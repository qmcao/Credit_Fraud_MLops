{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOAL: Monitering Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define Random State\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing all the data before spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fraud.csv')\n",
    "df.drop(['prev_address_months_count', 'intended_balcon_amount', 'device_fraud_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['month'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting the data by month\n",
    "\n",
    "source: https://arxiv.org/pdf/2401.05240v2\n",
    "\n",
    "According to the paper's methodology, we split by month:\n",
    "  - Training: months 1, 2, 3, 4, 5\n",
    "  - Validation: month 6\n",
    "  - Test: months 7, 8\n",
    " - [Ref: Jesus et al., 2022 - \"training set: month 1-5, validation set: month 6, testing set: months 7-8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set months: [1 2 3 4 5]\n",
      "Val   set months: [6]\n",
      "Test  set months: [7]\n",
      "Shapes:\n",
      " X_train: (662549, 28)  y_train: (662549,)\n",
      " X_val:   (108168, 28)  y_val:   (108168,)\n",
      " X_test:  (96843, 28)  y_test:  (96843,)\n"
     ]
    }
   ],
   "source": [
    "# Split your dataset BEFORE fitting any preprocessing to avoid data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_months = [1, 2, 3, 4, 5]\n",
    "val_month = [6]\n",
    "test_months = [7, 8]\n",
    "\n",
    "df_train = df[df[\"month\"].isin(train_months)]\n",
    "df_val = df[df['month'].isin(val_month)]\n",
    "df_test = df[df['month'].isin(test_months)]\n",
    "\n",
    "# Print quick info for sanity check\n",
    "print(\"Train set months:\", df_train['month'].unique())\n",
    "print(\"Val   set months:\", df_val['month'].unique())\n",
    "print(\"Test  set months:\", df_test['month'].unique())\n",
    "\n",
    "# Define target variable\n",
    "target_variable = 'fraud_bool'\n",
    "\n",
    "X_train = df_train.drop(columns=target_variable, axis=1)\n",
    "X_val = df_val.drop(columns=target_variable, axis=1)\n",
    "X_test = df_test.drop(columns=target_variable, axis=1)\n",
    "\n",
    "y_train = df_train[target_variable]\n",
    "y_val = df_val[target_variable]\n",
    "y_test = df_test[target_variable]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\" X_train:\", X_train.shape, \" y_train:\", y_train.shape)\n",
    "print(\" X_val:  \", X_val.shape,   \" y_val:  \", y_val.shape)\n",
    "print(\" X_test: \", X_test.shape,  \" y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature types\n",
    "\n",
    "num_features = ['income', 'name_email_similarity', 'current_address_months_count', 'customer_age', 'days_since_request'\n",
    "                    , 'zip_count_4w', 'velocity_6h', 'velocity_24h', 'velocity_4w', 'bank_branch_count_8w', \n",
    "                    'date_of_birth_distinct_emails_4w', 'credit_risk_score', 'bank_months_count', 'proposed_credit_limit',  'session_length_in_minutes',\n",
    "                    'device_distinct_emails_8w', 'month']\n",
    "\n",
    "\n",
    "cat_features = ['payment_type', 'employment_status', 'housing_status',\n",
    "                         'source', 'device_os']\n",
    "\n",
    "binary_features = [\n",
    "    'email_is_free',\n",
    "    'phone_home_valid',\n",
    "    'phone_mobile_valid',\n",
    "    'has_other_cards',\n",
    "    'foreign_request',\n",
    "    'keep_alive_session',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28452\\1828987840.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Transform numeric features on X_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m X_train_numeric = pd.DataFrame(\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mnum_imputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                 \u001b[0mmask_valid_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmissing_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_statistics_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m             \u001b[0mn_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_valid_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_statistics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m             \u001b[0mcoordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_valid_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2324\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2325\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# ------------------------------------------Numerical----------------------------------------\n",
    "# a. Impute Numerical Features\n",
    "# Median imputation is preferred when the distribution is skewed\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "num_imputer.fit(X_train[num_features])\n",
    "\n",
    "# Transform on training, validation data and test data\n",
    "\n",
    "# Transform numeric features on X_train \n",
    "X_train_numeric = pd.DataFrame(\n",
    "    num_imputer.transform(X_train[num_features]),\n",
    "    columns=num_features,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# Transform numeric features on X_val \n",
    "X_val_numeric = pd.DataFrame(\n",
    "    num_imputer.transform(X_val[num_features]),\n",
    "    columns=num_features,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "# Transform numeric features on X_test \n",
    "X_test_numeric = pd.DataFrame(\n",
    "    num_imputer.transform(X_test[num_features]),\n",
    "    columns=num_features,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# ------------------------------------------Categorical----------------------------------------\n",
    "# b. Impute Categorical Features\n",
    "# Initialize the imputer for categorical features\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "categorical_imputer.fit(X_train[cat_features])\n",
    "\n",
    "# # Transform on training, validation data and test data\n",
    "# Transform categorical features on X_train \n",
    "X_train_categorical = pd.DataFrame(\n",
    "    categorical_imputer.transform(X_train[cat_features]),\n",
    "    columns=cat_features,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# Transform categorical features on X_val\n",
    "X_val_categorical = pd.DataFrame(\n",
    "    categorical_imputer.transform(X_val[cat_features]),\n",
    "    columns=cat_features,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "# Transform categorical features on X_test\n",
    "X_test_categorical = pd.DataFrame(\n",
    "    categorical_imputer.transform(X_test[cat_features]),\n",
    "    columns=cat_features,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# ------------------------------------------Binary----------------------------------------\n",
    "# c. Impute Binary Features\n",
    "\n",
    "# Initialize the imputer for binary features\n",
    "binary_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "binary_imputer.fit(X_train[binary_features])\n",
    "\n",
    "# Transform on train, validation and test data\n",
    "# Transform binary features on X_train\n",
    "X_train_bin = pd.DataFrame(\n",
    "    binary_imputer.transform(X_train[binary_features]),\n",
    "    columns=binary_features,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# Transform binary features on X_val\n",
    "X_val_bin = pd.DataFrame(\n",
    "    binary_imputer.transform(X_val[binary_features]),\n",
    "    columns=binary_features,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "# Transform binary features on X_test\n",
    "X_test_bin = pd.DataFrame(\n",
    "    binary_imputer.transform(X_test[binary_features]),\n",
    "    columns=binary_features,\n",
    "    index=X_test.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numerical and categorical features and binary features\n",
    "X_train_imputed = pd.concat([X_train_numeric, X_train_categorical, X_train_bin], axis=1)\n",
    "X_val_imputed = pd.concat([X_val_numeric, X_val_categorical, X_val_bin], axis=1)\n",
    "X_test_imputed = pd.concat([X_test_numeric, X_test_categorical, X_test_bin], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every step is base on notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "# ------------------------------------------Numerical----------------------------------------\n",
    "# a. Scale Numerical Features\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train_imputed[num_features])\n",
    "\n",
    "# Transform on training, validating and testing data\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train_imputed[num_features]),\n",
    "    columns=num_features,\n",
    "    index=X_train_imputed.index\n",
    ")\n",
    "\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val_imputed[num_features]),\n",
    "    columns=num_features,\n",
    "    index=X_val_imputed.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_imputed[num_features]),\n",
    "    columns=num_features,\n",
    "    index=X_test_imputed.index\n",
    ")\n",
    "\n",
    "# ------------------------------------------Categorical----------------------------------------\n",
    "# b. Encode Categorical Features\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit the encoder on the training data\n",
    "encoder.fit(X_train_imputed[cat_features])\n",
    "\n",
    "# Transform on training, validating and testing data\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    encoder.transform(X_train_imputed[cat_features]),\n",
    "    columns=encoder.get_feature_names_out(cat_features),\n",
    "    index=X_train_imputed.index\n",
    ")\n",
    "\n",
    "X_val_encoded = pd.DataFrame(\n",
    "    encoder.transform(X_val_imputed[cat_features]),\n",
    "    columns=encoder.get_feature_names_out(cat_features),\n",
    "    index=X_val_imputed.index\n",
    ")\n",
    "\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    encoder.transform(X_test_imputed[cat_features]),\n",
    "    columns=encoder.get_feature_names_out(cat_features),\n",
    "    index=X_test_imputed.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed validation Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(108168, 49)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine scaled numerical and encoded categorical features\n",
    "X_train_preprocessed = pd.concat([X_train_scaled, X_train_encoded, X_train_bin], axis=1)\n",
    "X_val_preprocessed = pd.concat([X_val_scaled, X_val_encoded, X_val_bin], axis=1)\n",
    "X_test_preprocessed = pd.concat([X_test_scaled, X_test_encoded, X_test_bin], axis=1)\n",
    "\n",
    "print(\"Preprocessed validation Data:\")\n",
    "X_val_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection with mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chose top 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28452\\1275658607.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Prepare numerical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmutual_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmutual_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                     )\n\u001b[0;32m    213\u001b[0m                 ):\n\u001b[1;32m--> 214\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_classif\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \"\"\"\n\u001b[0;32m    489\u001b[0m     \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_estimate_mi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    302\u001b[0m         )\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     mi = [\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[0m_compute_mi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_feature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iterate_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     mi = [\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0m_compute_mi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_feature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iterate_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscrete_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     ]\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mx_discrete\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_discrete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py\u001b[0m in \u001b[0;36m_compute_mi_cd\u001b[1;34m(c, d, n_neighbors)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0mradius\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnextafter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mk_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    875\u001b[0m                     \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m                 )\n\u001b[1;32m--> 877\u001b[1;33m             chunked_results = Parallel(n_jobs, prefer=\"threads\")(\n\u001b[0m\u001b[0;32m    878\u001b[0m                 delayed(_tree_query_parallel_helper)(\n\u001b[0;32m    879\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[1;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \"\"\"\n\u001b[1;32m--> 683\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# a. Mutual information \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "#Prepare numerical features\n",
    "\n",
    "mutual_info = mutual_info_classif(X_train_preprocessed, y_train)\n",
    "mutual_info\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train_preprocessed.columns\n",
    "mutual_info.sort_values(ascending=False)\n",
    "\n",
    "# Choosing top 30 features\n",
    "feature_to_drop = mutual_info.sort_values(ascending=False)[31:].index.tolist()\n",
    "\n",
    "X_train_preprocessed.drop(columns=feature_to_drop, axis=1, inplace=True)\n",
    "X_val_preprocessed.drop(columns=feature_to_drop, axis=1, inplace=True)\n",
    "X_test_preprocessed.drop(columns=feature_to_drop, axis=1, inplace=True)\n",
    "X_train_preprocessed.shape,X_val_preprocessed, X_test_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training, assesing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model\n",
    "from lightgbm import LGBMClassifier as lgb\n",
    "\n",
    "# Scoring metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# import for oversampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "def find_best_threshold_for_max_recall_at_fpr(\n",
    "    val_probs, y_val, \n",
    "    target_fpr=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Given validation set probabilities (val_probs) and ground truth (y_val),\n",
    "    find the threshold that yields the highest recall subject to FPR <= target_fpr.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    val_probs : np.ndarray\n",
    "        Predicted probabilities for the positive class on the validation set.\n",
    "    y_val : np.ndarray\n",
    "        True labels (0 or 1) for the validation set.\n",
    "    target_fpr : float\n",
    "        The maximum allowed false positive rate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_threshold : float\n",
    "        The threshold that yields the maximum recall while keeping FPR <= target_fpr.\n",
    "    best_fpr : float\n",
    "        The FPR at that threshold.\n",
    "    best_recall : float\n",
    "        The recall at that threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the probabilities in ascending order\n",
    "    sorted_thresholds = np.sort(val_probs)\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_fpr = 1.0\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    # We'll try each threshold in ascending order.\n",
    "    # For each threshold, we measure FPR and recall.\n",
    "    for t in sorted_thresholds:\n",
    "        preds = (val_probs >= t).astype(int)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, preds, labels=[0,1]).ravel()\n",
    "        \n",
    "        # Compute FPR = FP / (FP + TN)\n",
    "        # Avoid division by zero if TN+FP=0\n",
    "        denom = (fp + tn) if (fp+tn) else 1e-15\n",
    "        current_fpr = fp / denom\n",
    "        \n",
    "        # Compute recall = TP / (TP + FN)\n",
    "        # Avoid division by zero if TP+FN=0\n",
    "        denom_pos = (tp + fn) if (tp+fn) else 1e-15\n",
    "        current_recall = tp / denom_pos\n",
    "        \n",
    "        # We only consider thresholds where FPR <= target_fpr\n",
    "        if current_fpr <= target_fpr:\n",
    "            # Among those, pick the one that yields the highest recall\n",
    "            if current_recall > best_recall:\n",
    "                best_threshold = t\n",
    "                best_fpr = current_fpr\n",
    "                best_recall = current_recall\n",
    "    \n",
    "    return best_threshold, best_fpr, best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier as lgb\n",
    "\n",
    "#Grid SearchCV\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Bayesian Optimization\n",
    "import optuna\n",
    "\n",
    "# Calibration library\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def run_experiment(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    n_bootstraps=5,\n",
    "    oversampling=False,\n",
    "    random_seed=42,\n",
    "    target_fpr=0.2  # Desired FPR for validation\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a training+evaluation experiment across multiple bootstraps, fixing\n",
    "    a threshold that yields <= target FPR on the validation set, and then\n",
    "    measuring precision, recall, and FPR on the test set at that threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : Training features and labels (DataFrame or Series)\n",
    "    X_val,   y_val   : Validation features and labels\n",
    "    X_test,  y_test  : Test features and labels\n",
    "    n_bootstraps     : Number of bootstrap iterations\n",
    "    oversampling     : Whether to apply SMOTE oversampling in each bootstrap\n",
    "    random_seed      : Random seed for reproducibility\n",
    "    target_fpr       : Desired FPR (false positive rate) on the validation set\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    precision_list, recall_list, fpr_list : Lists of precision, recall, and FPR across bootstraps\n",
    "    \"\"\"\n",
    "    \n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    fpr_list = []\n",
    "    decision_threshold = None  # We'll set this only in the first bootstrap\n",
    "    \n",
    "    np.random.seed(random_seed)  # Reproducibility\n",
    "    \n",
    "    best_params = {} # We'll set this only in the first bootstrap\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        print(f\"\\n=== Bootstrap Iteration #{i} (Oversampling={oversampling}) ===\")\n",
    "        \n",
    "        # ---------------------------\n",
    "        # 1) Bootstrapping\n",
    "        # ---------------------------\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_train_boot = X_train.iloc[indices]\n",
    "        y_train_boot = y_train.iloc[indices]\n",
    "\n",
    "        \n",
    "        # \n",
    "        #  ---------------------------\n",
    "        # 2) (Optional) Oversampling\n",
    "        # ---------------------------\n",
    "        if oversampling:\n",
    "            smote_obj = SMOTE(random_state=i)  # different random state each iteration\n",
    "            X_train_boot, y_train_boot = smote_obj.fit_resample(X_train_boot, y_train_boot)\n",
    "            \n",
    "        # ---------------------------\n",
    "        # 3) Hyper Parameter Tuning for first boostraps using validation set (month 6)\n",
    "        # ---------------------------        \n",
    "        if i == 0:\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'auc',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "                    'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n",
    "                    'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n",
    "                    'scale_pos_weight': len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "                }\n",
    "                \n",
    "                # --- Classifier ---\n",
    "                optuna_model = lgb(random_state = i)\n",
    "                optuna_model.set_params(**params)\n",
    "                #fit model\n",
    "                optuna_model.fit(\n",
    "                    X=X_train_boot,\n",
    "                    y=y_train_boot,\n",
    "                )\n",
    "                \n",
    "                # Predict on the validation set\n",
    "                y_pred_proba = optuna_model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "                # Return the validation ROC_AUC score as the optimization objective\n",
    "                score = roc_auc_score(y_val, y_pred_proba)\n",
    "                \n",
    "                \n",
    "                return score.mean()\n",
    "\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(objective, n_trials=30)\n",
    "                    \n",
    "            best_params = study.best_params\n",
    "        \n",
    "        # ---------------------------\n",
    "        # 4) Model Training with best param from first iterations\n",
    "        # ---------------------------\n",
    "        \n",
    "        print(best_params)\n",
    "        tuned_lgb = lgb(random_state=i)\n",
    "        tuned_lgb.set_params(**best_params)\n",
    "        tuned_lgb.fit(X_train_boot, y_train_boot)     \n",
    "        \n",
    "        # ---------------------------\n",
    "        # 5) Calibration (Platt / sigmoid) on the 1st bootstrap with validation set set to define threshold. \n",
    "        # We then reapply the same threshold for the remaining bootstraps\n",
    "        # ---------------------------\n",
    "        if i == 0:\n",
    "            # calibrated_clf = CalibratedClassifierCV(\n",
    "            #     base_estimator=base_lgb, cv='prefit', method='sigmoid') #this code is wrong, base_estimator is no longer support!, takes 3 hours to debug\n",
    "            calibrated_clf = CalibratedClassifierCV(\n",
    "                estimator=tuned_lgb, cv=\"prefit\", method=\"sigmoid\"\n",
    "            )\n",
    "            calibrated_clf.fit(X_val, y_val)        \n",
    "            \n",
    "            val_probs = calibrated_clf.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # Maximum recall score at desired FPR\n",
    "            threshold, fpr, recall_ = find_best_threshold_for_max_recall_at_fpr(\n",
    "                val_probs, \n",
    "                y_val, \n",
    "                target_fpr=target_fpr\n",
    "            )\n",
    "            \n",
    "            decision_threshold = threshold\n",
    "            print(f\"Chosen decision threshold (FPR <= {target_fpr}) = {decision_threshold:.3f}\")\n",
    "            print(f\"Resulting FPR on validation set: {fpr:.3f}\")\n",
    "            print(f\"Resulting Recall on validation set: {recall_:.3f}\")\n",
    "        \n",
    "        # ---------------------------\n",
    "        # 6) Evaluate on Test set for Model #k > 0\n",
    "        # ---------------------------       \n",
    "        else:\n",
    "            test_probs = tuned_lgb.predict_proba(X_test)[:, 1]\n",
    "            test_preds = (test_probs > decision_threshold).astype(int)\n",
    "            \n",
    "            # Precision / Recall\n",
    "            prec = precision_score(y_test, test_preds)\n",
    "            rec = recall_score(y_test, test_preds)\n",
    "            \n",
    "            # FPR on Test\n",
    "            tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, test_preds, labels=[0,1]).ravel()\n",
    "            denom_test = (tn_test + fp_test) if (tn_test + fp_test) else 1e-10\n",
    "            test_fpr = fp_test / denom_test\n",
    "            \n",
    "            precision_list.append(prec)\n",
    "            recall_list.append(rec)\n",
    "            fpr_list.append(test_fpr)\n",
    "            \n",
    "            print(f\"Test Precision @ threshold={decision_threshold:.3f}: {prec:.3f}\")\n",
    "            print(f\"Test Recall    @ threshold={decision_threshold:.3f}: {rec:.3f}\")\n",
    "            print(f\"Test FPR       @ threshold={decision_threshold:.3f}: {test_fpr:.3f}\")\n",
    "    \n",
    "    return precision_list, recall_list, fpr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT 1: No Oversampling ===\n",
      "\n",
      "=== Bootstrap Iteration #0 (Oversampling=False) ===\n",
      "Iteration: 0  Class distribution: [655799   6750]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call fit before exploiting the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28452\\1620773368.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# ---------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=== EXPERIMENT 1: No Oversampling ===\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m precision_no_os, recall_no_os, fpr_no_os = run_experiment(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX_train_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mn_bootstraps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28452\\4261808588.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, y_test, n_bootstraps, oversampling, random_seed, target_fpr)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# previously use fit, does not work for calling calibrated_clf.fit, change to predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# source : https://stackoverflow.com/questions/60835032/lgbmclassifier-has-no-attribute-apply-probability-calibration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mbase_lgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_boot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     ):\n\u001b[0;32m   1320\u001b[0m         \u001b[1;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         result = self.predict_proba(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     ):\n\u001b[0;32m   1350\u001b[0m         \u001b[1;34m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1351\u001b[1;33m         result = super().predict(\n\u001b[0m\u001b[0;32m   1352\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[1;34m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__sklearn_is_fitted__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mLGBMNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Estimator not fitted, call fit before exploiting the model.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpd_DataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_DataTable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_LGBMCheckArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Estimator not fitted, call fit before exploiting the model."
     ]
    }
   ],
   "source": [
    "# C) Run Experiments (Oversample vs. No Oversample)\n",
    "# ---------------------------------\n",
    "print(\"=== EXPERIMENT 1: No Oversampling ===\")\n",
    "precision_no_os, recall_no_os, fpr_no_os = run_experiment(\n",
    "    X_train_preprocessed, y_train, X_val_preprocessed, y_val, X_test_preprocessed, y_test,\n",
    "    n_bootstraps=5,\n",
    "    oversampling=False,\n",
    "    random_seed=42,\n",
    "    target_fpr = 0.2\n",
    ")\n",
    "\n",
    "print(\"\\n=== EXPERIMENT 2: With Oversampling ===\")\n",
    "precision_os, recall_os, fpr_os = run_experiment(\n",
    "    X_train_preprocessed, y_train, X_val_preprocessed, y_val, X_test_preprocessed, y_test,\n",
    "    n_bootstraps=5,\n",
    "    oversampling=True,\n",
    "    random_seed=42,\n",
    "    target_fpr= 0.2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARISON OF RESULTS ===\n",
      "\n",
      "No Oversampling => Precision: nan ± nan, Recall: nan ± nan FPR: nan ± nan\n",
      "Oversampling   => Precision: 0.066 ± 0.003, Recall: 0.773 ± 0.015 FPR: 0.165 ± 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "d:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "d:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "d:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "d:\\project\\Credit_Fraud_endToEnd\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# D) Compare Results\n",
    "# ---------------------------\n",
    "print(\"\\n=== COMPARISON OF RESULTS ===\")\n",
    "\n",
    "mean_prec_no_os = np.mean(precision_no_os)\n",
    "std_prec_no_os = np.std(precision_no_os)\n",
    "mean_rec_no_os = np.mean(recall_no_os)\n",
    "std_rec_no_os = np.std(recall_no_os)\n",
    "mean_fpr_no_os = np.mean(fpr_no_os)\n",
    "std_fpr_no_os = np.std(fpr_no_os)\n",
    "\n",
    "mean_prec_os = np.mean(precision_os)\n",
    "std_prec_os = np.std(precision_os)\n",
    "mean_rec_os = np.mean(recall_os)\n",
    "std_rec_os = np.std(recall_os)\n",
    "mean_fpr_os = np.mean(fpr_os)\n",
    "std_fpr_os = np.std(fpr_os)\n",
    "\n",
    "print(f\"\\nNo Oversampling => Precision: {mean_prec_no_os:.3f} ± {std_prec_no_os:.3f}, \"\n",
    "        f\"Recall: {mean_rec_no_os:.3f} ± {std_rec_no_os:.3f}\",\n",
    "        f\"FPR: {mean_fpr_no_os:.3f} ± {std_fpr_no_os:.3f}\"\n",
    "        )\n",
    "print(f\"Oversampling   => Precision: {mean_prec_os:.3f} ± {std_prec_os:.3f}, \"\n",
    "        f\"Recall: {mean_rec_os:.3f} ± {std_rec_os:.3f}\", \n",
    "        f\"FPR: {mean_fpr_os:.3f} ± {std_fpr_os:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is not much different in performance of oversampling and no sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT 3: No Oversampling with calibration ===\n",
      "\n",
      "=== Bootstrap Iteration #0 (Oversampling=False) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 15:28:27,190] A new study created in memory with name: no-name-89d1fe97-cd79-4f7f-90dd-2de0bfb31ea3\n",
      "[I 2025-01-13 15:28:29,457] Trial 0 finished with value: 0.8679363368878728 and parameters: {'num_leaves': 80, 'max_depth': 3, 'learning_rate': 0.11539323601758196, 'subsample': 0.9797345919014896, 'colsample_bytree': 0.9011778952395055, 'lambda_l1': 7.647145012886648, 'lambda_l2': 2.60561135890534}. Best is trial 0 with value: 0.8679363368878728.\n",
      "[I 2025-01-13 15:28:32,158] Trial 1 finished with value: 0.872041319339206 and parameters: {'num_leaves': 52, 'max_depth': 8, 'learning_rate': 0.04987591264014918, 'subsample': 0.7432374573533561, 'colsample_bytree': 0.8287650854960851, 'lambda_l1': 6.066580703406823, 'lambda_l2': 7.787688902390155}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:34,780] Trial 2 finished with value: 0.7586508917152586 and parameters: {'num_leaves': 80, 'max_depth': 12, 'learning_rate': 0.1982747479774949, 'subsample': 0.8332097563533905, 'colsample_bytree': 0.7048000189324152, 'lambda_l1': 5.853149952861446, 'lambda_l2': 9.565882875673914}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:37,154] Trial 3 finished with value: 0.8474332740299765 and parameters: {'num_leaves': 52, 'max_depth': 9, 'learning_rate': 0.12700924926062554, 'subsample': 0.8679238914056966, 'colsample_bytree': 0.8443308847726356, 'lambda_l1': 2.0392491492137843, 'lambda_l2': 5.792290886205357}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:39,233] Trial 4 finished with value: 0.7682535699953018 and parameters: {'num_leaves': 93, 'max_depth': 5, 'learning_rate': 0.17496925148553719, 'subsample': 0.9431822685771087, 'colsample_bytree': 0.8916211825372099, 'lambda_l1': 8.669569221499755, 'lambda_l2': 9.79005310022494}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:42,067] Trial 5 finished with value: 0.8625487281659495 and parameters: {'num_leaves': 99, 'max_depth': 10, 'learning_rate': 0.06332811835351895, 'subsample': 0.7729683245087738, 'colsample_bytree': 0.8250582124376769, 'lambda_l1': 5.798594900177677, 'lambda_l2': 7.3135965255457585}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:43,994] Trial 6 finished with value: 0.7447046001353228 and parameters: {'num_leaves': 43, 'max_depth': 10, 'learning_rate': 0.1591690363901045, 'subsample': 0.7630290245153234, 'colsample_bytree': 0.9659109968220523, 'lambda_l1': 9.423948778284426, 'lambda_l2': 5.548410562602884}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:46,014] Trial 7 finished with value: 0.8628932972558679 and parameters: {'num_leaves': 31, 'max_depth': 5, 'learning_rate': 0.0735113192392176, 'subsample': 0.9229487341576204, 'colsample_bytree': 0.7006350405462454, 'lambda_l1': 4.683619188215945, 'lambda_l2': 5.041448337084656}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:48,883] Trial 8 finished with value: 0.8555021968953304 and parameters: {'num_leaves': 89, 'max_depth': 12, 'learning_rate': 0.09431639872449522, 'subsample': 0.8495933606789545, 'colsample_bytree': 0.9801720087874549, 'lambda_l1': 7.479244756280653, 'lambda_l2': 8.932860441451332}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:51,619] Trial 9 finished with value: 0.7426797179288501 and parameters: {'num_leaves': 77, 'max_depth': 10, 'learning_rate': 0.17810605655979397, 'subsample': 0.8965519844597507, 'colsample_bytree': 0.7416432665895613, 'lambda_l1': 5.343585538685679, 'lambda_l2': 4.407520426907318}. Best is trial 1 with value: 0.872041319339206.\n",
      "[I 2025-01-13 15:28:53,824] Trial 10 finished with value: 0.8732096417823061 and parameters: {'num_leaves': 22, 'max_depth': 7, 'learning_rate': 0.01042921789999264, 'subsample': 0.7040152334103023, 'colsample_bytree': 0.7976840566306402, 'lambda_l1': 0.14513106865102987, 'lambda_l2': 0.18302831844699075}. Best is trial 10 with value: 0.8732096417823061.\n",
      "[I 2025-01-13 15:28:56,010] Trial 11 finished with value: 0.8739876671420845 and parameters: {'num_leaves': 21, 'max_depth': 7, 'learning_rate': 0.014769197570200761, 'subsample': 0.7205900408378844, 'colsample_bytree': 0.7791577615696796, 'lambda_l1': 0.9711588175005677, 'lambda_l2': 0.05734132688600832}. Best is trial 11 with value: 0.8739876671420845.\n",
      "[I 2025-01-13 15:28:58,209] Trial 12 finished with value: 0.8733197708947396 and parameters: {'num_leaves': 21, 'max_depth': 7, 'learning_rate': 0.01464934922718218, 'subsample': 0.7040261801819536, 'colsample_bytree': 0.7745004679720445, 'lambda_l1': 0.20987252000833728, 'lambda_l2': 0.20908911348758213}. Best is trial 11 with value: 0.8739876671420845.\n",
      "[I 2025-01-13 15:29:00,432] Trial 13 finished with value: 0.8732366029451775 and parameters: {'num_leaves': 22, 'max_depth': 6, 'learning_rate': 0.011905110456059876, 'subsample': 0.7007417302458536, 'colsample_bytree': 0.7776995972682761, 'lambda_l1': 0.35861081602549144, 'lambda_l2': 0.5004156265457758}. Best is trial 11 with value: 0.8739876671420845.\n",
      "[I 2025-01-13 15:29:02,846] Trial 14 finished with value: 0.8753423427906354 and parameters: {'num_leaves': 34, 'max_depth': 14, 'learning_rate': 0.03621113618270589, 'subsample': 0.8110477359626707, 'colsample_bytree': 0.7664981515138998, 'lambda_l1': 2.318068775565718, 'lambda_l2': 1.8384217775544631}. Best is trial 14 with value: 0.8753423427906354.\n",
      "[I 2025-01-13 15:29:05,293] Trial 15 finished with value: 0.8718103593680026 and parameters: {'num_leaves': 36, 'max_depth': 14, 'learning_rate': 0.03971190636266317, 'subsample': 0.8052802390348538, 'colsample_bytree': 0.7426482753449907, 'lambda_l1': 2.9891012400607013, 'lambda_l2': 2.049176517306292}. Best is trial 14 with value: 0.8753423427906354.\n",
      "[I 2025-01-13 15:29:07,787] Trial 16 finished with value: 0.8742158482781885 and parameters: {'num_leaves': 35, 'max_depth': 15, 'learning_rate': 0.03561858387831861, 'subsample': 0.8037863380502287, 'colsample_bytree': 0.8787877464844424, 'lambda_l1': 1.9942280888754595, 'lambda_l2': 2.408425468682134}. Best is trial 14 with value: 0.8753423427906354.\n",
      "[I 2025-01-13 15:29:10,272] Trial 17 finished with value: 0.8582869386349199 and parameters: {'num_leaves': 63, 'max_depth': 15, 'learning_rate': 0.08531648921135644, 'subsample': 0.8053037631862268, 'colsample_bytree': 0.891913240851997, 'lambda_l1': 3.4795993476905265, 'lambda_l2': 2.5908378004564008}. Best is trial 14 with value: 0.8753423427906354.\n",
      "[I 2025-01-13 15:29:12,781] Trial 18 finished with value: 0.8772006402953062 and parameters: {'num_leaves': 36, 'max_depth': 13, 'learning_rate': 0.04144173155842981, 'subsample': 0.806891370619104, 'colsample_bytree': 0.9216964780167786, 'lambda_l1': 2.0728380318065036, 'lambda_l2': 3.7376966509737857}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:15,441] Trial 19 finished with value: 0.8706355066624186 and parameters: {'num_leaves': 64, 'max_depth': 12, 'learning_rate': 0.05386371165514191, 'subsample': 0.8869201187565274, 'colsample_bytree': 0.9437832712063121, 'lambda_l1': 3.9671842341649857, 'lambda_l2': 3.6178444570444257}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:17,528] Trial 20 finished with value: 0.7907975385983426 and parameters: {'num_leaves': 45, 'max_depth': 13, 'learning_rate': 0.13197132348325205, 'subsample': 0.8339271301846547, 'colsample_bytree': 0.9290631940157942, 'lambda_l1': 2.2938133983599744, 'lambda_l2': 1.5188734173759775}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:20,000] Trial 21 finished with value: 0.8727853685930888 and parameters: {'num_leaves': 32, 'max_depth': 15, 'learning_rate': 0.03784803999894987, 'subsample': 0.7993669819464831, 'colsample_bytree': 0.8640428872985239, 'lambda_l1': 1.4870965921456856, 'lambda_l2': 3.602489336240958}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:22,580] Trial 22 finished with value: 0.8745048083540831 and parameters: {'num_leaves': 42, 'max_depth': 14, 'learning_rate': 0.032984602098787966, 'subsample': 0.7877423021275427, 'colsample_bytree': 0.8666736959163669, 'lambda_l1': 2.6146189686644856, 'lambda_l2': 1.433793557471171}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:24,924] Trial 23 finished with value: 0.8660127787640128 and parameters: {'num_leaves': 42, 'max_depth': 13, 'learning_rate': 0.07048094530040183, 'subsample': 0.7722207641248877, 'colsample_bytree': 0.915750977063833, 'lambda_l1': 4.169811257307741, 'lambda_l2': 1.4728873254087596}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:28,167] Trial 24 finished with value: 0.8734483210989195 and parameters: {'num_leaves': 50, 'max_depth': 13, 'learning_rate': 0.030001674943729573, 'subsample': 0.7459477232501689, 'colsample_bytree': 0.8580516384489626, 'lambda_l1': 2.9040400737400387, 'lambda_l2': 3.570888244632873}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:31,008] Trial 25 finished with value: 0.8738713502747493 and parameters: {'num_leaves': 29, 'max_depth': 14, 'learning_rate': 0.05304535176796206, 'subsample': 0.8272471559829644, 'colsample_bytree': 0.9551779554532478, 'lambda_l1': 1.2834581520974364, 'lambda_l2': 1.5182055691994267}. Best is trial 18 with value: 0.8772006402953062.\n",
      "[I 2025-01-13 15:29:33,969] Trial 26 finished with value: 0.8776449792589041 and parameters: {'num_leaves': 56, 'max_depth': 11, 'learning_rate': 0.028264344750403514, 'subsample': 0.7827563033434293, 'colsample_bytree': 0.9996985528187515, 'lambda_l1': 2.8892887717835674, 'lambda_l2': 3.257513551038251}. Best is trial 26 with value: 0.8776449792589041.\n",
      "[I 2025-01-13 15:29:36,590] Trial 27 finished with value: 0.8594186483099836 and parameters: {'num_leaves': 58, 'max_depth': 11, 'learning_rate': 0.08611933049089177, 'subsample': 0.8626662316520471, 'colsample_bytree': 0.9689750173070473, 'lambda_l1': 3.7747565624411954, 'lambda_l2': 3.2066943635759806}. Best is trial 26 with value: 0.8776449792589041.\n",
      "[I 2025-01-13 15:29:39,579] Trial 28 finished with value: 0.877552460206112 and parameters: {'num_leaves': 68, 'max_depth': 11, 'learning_rate': 0.02673852503177832, 'subsample': 0.7550711060613032, 'colsample_bytree': 0.9973908959270074, 'lambda_l1': 0.9711715668046335, 'lambda_l2': 4.3809366709528526}. Best is trial 26 with value: 0.8776449792589041.\n",
      "[I 2025-01-13 15:29:42,220] Trial 29 finished with value: 0.8398901067654294 and parameters: {'num_leaves': 71, 'max_depth': 11, 'learning_rate': 0.10945976212925698, 'subsample': 0.7445286120766502, 'colsample_bytree': 0.9973070082476023, 'lambda_l1': 0.9104426141292444, 'lambda_l2': 4.4968135513530285}. Best is trial 26 with value: 0.8776449792589041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_leaves': 56, 'max_depth': 11, 'learning_rate': 0.028264344750403514, 'subsample': 0.7827563033434293, 'colsample_bytree': 0.9996985528187515, 'lambda_l1': 2.8892887717835674, 'lambda_l2': 3.257513551038251}\n",
      "Chosen decision threshold (FPR <= 0.2) = 0.010\n",
      "Resulting FPR on validation set: 0.200\n",
      "Resulting Recall on validation set: 0.807\n",
      "\n",
      "=== Bootstrap Iteration #1 (Oversampling=False) ===\n",
      "{'num_leaves': 56, 'max_depth': 11, 'learning_rate': 0.028264344750403514, 'subsample': 0.7827563033434293, 'colsample_bytree': 0.9996985528187515, 'lambda_l1': 2.8892887717835674, 'lambda_l2': 3.257513551038251}\n",
      "Test Precision @ threshold=0.010: 0.064\n",
      "Test Recall    @ threshold=0.010: 0.783\n",
      "Test FPR       @ threshold=0.010: 0.172\n",
      "\n",
      "=== Bootstrap Iteration #2 (Oversampling=False) ===\n",
      "{'num_leaves': 56, 'max_depth': 11, 'learning_rate': 0.028264344750403514, 'subsample': 0.7827563033434293, 'colsample_bytree': 0.9996985528187515, 'lambda_l1': 2.8892887717835674, 'lambda_l2': 3.257513551038251}\n",
      "Test Precision @ threshold=0.010: 0.061\n",
      "Test Recall    @ threshold=0.010: 0.798\n",
      "Test FPR       @ threshold=0.010: 0.184\n",
      "\n",
      "=== Bootstrap Iteration #3 (Oversampling=False) ===\n",
      "{'num_leaves': 56, 'max_depth': 11, 'learning_rate': 0.028264344750403514, 'subsample': 0.7827563033434293, 'colsample_bytree': 0.9996985528187515, 'lambda_l1': 2.8892887717835674, 'lambda_l2': 3.257513551038251}\n",
      "Test Precision @ threshold=0.010: 0.060\n",
      "Test Recall    @ threshold=0.010: 0.803\n",
      "Test FPR       @ threshold=0.010: 0.189\n",
      "\n",
      "=== Bootstrap Iteration #4 (Oversampling=False) ===\n",
      "{'num_leaves': 56, 'max_depth': 11, 'learning_rate': 0.028264344750403514, 'subsample': 0.7827563033434293, 'colsample_bytree': 0.9996985528187515, 'lambda_l1': 2.8892887717835674, 'lambda_l2': 3.257513551038251}\n",
      "Test Precision @ threshold=0.010: 0.056\n",
      "Test Recall    @ threshold=0.010: 0.809\n",
      "Test FPR       @ threshold=0.010: 0.203\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXPERIMENT 3: No Oversampling with calibration ===\")\n",
    "precision_no_os, recall_no_os, fpr_no_os = run_experiment(\n",
    "    X_train_preprocessed, y_train, X_val_preprocessed, y_val, X_test_preprocessed, y_test,\n",
    "    n_bootstraps=5,\n",
    "    oversampling=False,\n",
    "    random_seed=42,\n",
    "    target_fpr = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No Oversampling => Precision: 0.060 ± 0.003, Recall: 0.798 ± 0.010 FPR: 0.187 ± 0.011\n"
     ]
    }
   ],
   "source": [
    "mean_prec_no_os = np.mean(precision_no_os)\n",
    "std_prec_no_os = np.std(precision_no_os)\n",
    "mean_rec_no_os = np.mean(recall_no_os)\n",
    "std_rec_no_os = np.std(recall_no_os)\n",
    "mean_fpr_no_os = np.mean(fpr_no_os)\n",
    "std_fpr_no_os = np.std(fpr_no_os)\n",
    "\n",
    "\n",
    "print(f\"\\nNo Oversampling => Precision: {mean_prec_no_os:.3f} ± {std_prec_no_os:.3f}, \"\n",
    "        f\"Recall: {mean_rec_no_os:.3f} ± {std_rec_no_os:.3f}\",\n",
    "        f\"FPR: {mean_fpr_no_os:.3f} ± {std_fpr_no_os:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
